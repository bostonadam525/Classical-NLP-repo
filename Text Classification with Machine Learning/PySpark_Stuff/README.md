# PySpark Stuff
* This repo is everything related to big data processing in PySpark.
* See PySpark docs: https://spark.apache.org/docs/latest/api/python/index.html


# Speed of PySpark
* Run workloads 100x faster
* Uses state of the art DAG scheduler, query optimizer and physical execution engine.
* Apache Spark is 100x faster than Map Reduce.
* Key points about Spark's speed advantages:
  * 1. **In-memory processing**
    * Spark stores intermediate data in RAM, allowing for much faster data access compared to disk-based MapReduce. 
  * 2. **Optimized for iterative tasks**
    * Spark is designed to efficiently handle iterative algorithms where data needs to be accessed multiple times, further enhancing performance. 
  * 3. **Improved parallelism**
  * Spark can effectively distribute tasks across multiple nodes in a cluster, maximizing parallel processing capabilities.
 

